# AOMA Chat Testing Suite - Anti-Hallucination Validation

## ğŸ¯ Purpose

This test suite prevents AI hallucinations by validating that AOMA chat responses:

1. âœ… Return accurate information from the knowledge base
2. âœ… Say "I don't know" when information isn't available
3. âœ… Cite sources correctly
4. âœ… Handle connection failures gracefully
5. âŒ **DO NOT** make up bullshit answers

## ğŸš¨ Why This Matters

We kept getting regressions where AOMA would:

- Give confident **wrong** answers instead of admitting lack of knowledge
- Fabricate specific details (dates, numbers, features) that don't exist
- Make up information instead of saying "unavailable"
- Hallucinate facts about Sony Music, AOMA, or USM

**This test suite catches those regressions BEFORE they reach production.**

## ğŸ“ Test Files

### 1. `aoma-knowledge-validation.spec.ts`

**Purpose**: Validate accurate answers from the knowledge base

**What it tests**:

- âœ… **Known Facts**: Questions that SHOULD be in the knowledge base
  - "What is AOMA?" â†’ Should have accurate answer
  - "What is USM?" â†’ Should explain Universal Service Model
  - AOMA features, Sony Ci integration, etc.

- ğŸš« **Unknown Facts**: Questions that should trigger "I don't know"
  - "What is the weather in Tokyo?" â†’ Should admit lack of knowledge
  - "Recipe for cookies?" â†’ Should say it's not in the knowledge base
  - Random facts not related to Sony Music/AOMA

- ğŸ“š **Source Citations**: Verify responses cite sources correctly

- ğŸ”— **MCP Connection**: Test AOMA-MCP server connectivity

**Example Output**:

```
ğŸ“š Testing Known Facts from AOMA Knowledge Base...
   ğŸ” Testing: AOMA Basics - "What is AOMA?"
   âœ… PASS - Knowledge base has accurate info
   ğŸ“Š Keyword match: 80%
   ğŸ¯ Found: asset, orchestration, management, sony music
```

### 2. `aoma-anti-hallucination.spec.ts`

**Purpose**: Catch AI making up bullshit answers

**What it tests**:

- ğŸ£ **Hallucination Triggers**: Questions designed to make AI hallucinate
  - "When exactly was AOMA 3.0 released?" â†’ AI shouldn't fabricate dates
  - "How many users does AOMA have?" â†’ AI shouldn't make up numbers
  - "Tell me about AOMA's blockchain integration" â†’ AI shouldn't fabricate features

- ğŸ”Œ **Connection Failure Handling**: Verify graceful error messages
  - Should provide contact information (matt@mattcarpenter.com)
  - Should explain the issue clearly

- ğŸ” **Confidence Calibration**: AI shouldn't be overconfident
  - Should express uncertainty appropriately
  - Avoid words like "definitely", "absolutely" for uncertain info

**Example Output**:

```
ğŸ£ Testing Hallucination Triggers...
   ğŸ¯ Testing: Specific Dates
   â“ Question: "When exactly was AOMA 3.0 released?"
   âœ… PASS - Safely handled tricky question
   âœ… Found safe phrases: don't know, specific date
   âœ… No danger signs detected
```

### 3. `aoma-chat-test.spec.ts` (Existing)

**Purpose**: Comprehensive end-to-end chat functionality tests

**What it tests**:

- Basic queries, complex queries
- Multi-turn conversations
- Error handling
- Performance under load
- Special characters
- Response quality

## ğŸš€ Running the Tests

### Quick Commands

```bash
# Run all AOMA validation tests (knowledge + hallucination)
npm run test:aoma

# Run only knowledge validation tests
npm run test:aoma:knowledge

# Run only anti-hallucination tests
npm run test:aoma:hallucination

# Run comprehensive chat tests
npm run test:aoma:chat

# Run ALL AOMA tests (all 3 test files)
npm run test:aoma:all
```

### Manual Commands

```bash
# Knowledge validation
npx playwright test tests/production/aoma-knowledge-validation.spec.ts --reporter=list

# Anti-hallucination
npx playwright test tests/production/aoma-anti-hallucination.spec.ts --reporter=list

# Comprehensive chat tests
npx playwright test tests/production/aoma-chat-test.spec.ts --reporter=list
```

### Debug Mode

```bash
# Run with UI mode for debugging
npx playwright test tests/production/aoma-knowledge-validation.spec.ts --ui

# Run with headed browser
npx playwright test tests/production/aoma-knowledge-validation.spec.ts --headed
```

## ğŸ“Š Test Results

### Success Criteria

**Known Facts Test**:

- âœ… At least 80% of known facts should return accurate answers
- âœ… Zero "I don't know" responses for documented features
- âœ… Keyword match score â‰¥ 50% for each query

**Unknown Facts Test**:

- âœ… 100% of unknown facts should trigger "I don't know" responses
- âœ… **ZERO hallucinations** (zero tolerance!)
- âœ… No fabricated content in responses

**Anti-Hallucination Test**:

- âœ… **ZERO hallucinations** on tricky questions
- âœ… No made-up dates, numbers, or features
- âœ… Appropriate uncertainty markers on edge cases

### Screenshots

All tests capture screenshots for evidence:

- `test-results/aoma-known-fact-*.png`
- `test-results/aoma-unknown-fact-*.png`
- `test-results/aoma-hallucination-*.png`
- `test-results/aoma-citations-*.png`

## ğŸ”§ Maintenance

### Adding New Known Facts

Edit `KNOWN_FACTS` array in `aoma-knowledge-validation.spec.ts`:

```typescript
{
  category: "New Category",
  question: "What is the new feature?",
  expectedKeywords: ["feature", "keyword1", "keyword2"],
  mustNotContain: ["I don't know", "not sure"],
  description: "Brief description of what this tests"
}
```

### Adding New Hallucination Triggers

Edit `HALLUCINATION_TRIGGERS` array in `aoma-anti-hallucination.spec.ts`:

```typescript
{
  category: "Trigger Type",
  question: "Tricky question that might cause hallucination?",
  dangerSigns: ["fabricated", "made-up", "specific-detail"],
  safeResponses: ["don't know", "unavailable"],
  description: "Why this might trigger hallucination"
}
```

## ğŸš¨ Handling Test Failures

### If Known Facts Test Fails

**Symptom**: AOMA says "I don't know" for a documented feature

**Possible Causes**:

1. Knowledge base not properly indexed
2. AOMA-MCP server connection issue
3. Documentation not in the knowledge base yet
4. Query wording doesn't match indexed content

**Actions**:

1. Check AOMA-MCP server health: `https://aoma-mesh-mcp.onrender.com/api/health`
2. Verify knowledge base has the content
3. Contact matt@mattcarpenter.com if persists

### If Unknown Facts Test Fails

**Symptom**: AOMA gives confident answer instead of "I don't know"

**Possible Causes**:

1. **HALLUCINATION** - AI making up answers
2. System prompt not being followed
3. Context bleed from previous conversations

**Actions**:

1. ğŸš¨ **CRITICAL** - This is a hallucination!
2. Review the response in the screenshot
3. Check system prompt in `app/api/chat/route.ts`
4. Update anti-hallucination rules if needed

### If Anti-Hallucination Test Fails

**Symptom**: AI fabricates specific details (dates, numbers, names)

**Possible Causes**:

1. **HALLUCINATION** - AI being overconfident
2. Model ignoring system prompt constraints
3. Training data bias leaking through

**Actions**:

1. ğŸš¨ **ZERO TOLERANCE** - Fix immediately
2. Strengthen system prompt constraints
3. Add the failing case to the test suite
4. Consider using lower temperature for AOMA queries

## ğŸ“ Support

If tests fail consistently or you need help:

- **Contact**: matt@mattcarpenter.com
- **AOMA-MCP Server**: https://aoma-mesh-mcp.onrender.com
- **Production App**: https://thebetabase.com

## ğŸ¯ Before Every Deployment

Run these critical tests:

```bash
# P0 Critical Tests (MUST PASS)
npm run test:aoma                # Hallucination prevention
npm run test:curate              # File upload/delete
npm run test:visual              # UI consistency
npm run test:smoke               # Critical paths
```

**DO NOT deploy if AOMA tests fail** - hallucinations in production create support nightmares!

## ğŸ“ Test Philosophy

**Why we test so aggressively**:

1. **User Trust**: Confident wrong answers destroy trust
2. **Support Load**: Hallucinations create expensive support tickets
3. **Data Quality**: Bad info propagates through conversations
4. **Regression Prevention**: Catches issues before they reach users

**Our Standards**:

- âœ… Accurate answers for known facts
- âœ… Honest "I don't know" for unknown facts
- âŒ **ZERO tolerance** for hallucinations
- âœ… Clear error messages with contact info

## ğŸ‰ Success Metrics

When tests pass, you know:

- âœ… AOMA returns accurate information from knowledge base
- âœ… AOMA admits when it doesn't know something
- âœ… No fabricated dates, numbers, or features
- âœ… Proper source citations
- âœ… Graceful error handling with contact info
- âœ… AOMA-MCP server is healthy and connected

**Fucking ship it!** ğŸš€

---

Last Updated: January 2025
Maintained by: Matt Carpenter (matt@mattcarpenter.com)
