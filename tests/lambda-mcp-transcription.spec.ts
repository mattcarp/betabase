/**
 * Lambda MCP Transcription Integration Tests
 * Comprehensive test suite for Lambda MCP + SIAM transcription pipeline
 *
 * Tests:
 * - Audio routing through Lambda MCP
 * - Transcription accuracy
 * - Timeout handling
 * - Fallback mechanisms
 * - End-to-end data flow
 * - Performance metrics
 */

import { test, expect } from "@playwright/test";
import fs from "fs";
import path from "path";

// Test configuration
const LAMBDA_MCP_ENDPOINT =
  process.env.LAMBDA_MCP_URL ||
  "https://ochwh4pvfaigb65koqxgf33ruy0rxnhy.lambda-url.us-east-2.on.aws";
const SIAM_BASE_URL = process.env.BASE_URL || "http://localhost:3000";

test.describe("Lambda MCP Transcription Pipeline", () => {
  test.beforeEach(async ({ page }) => {
    // Navigate to the application
    await page.goto(SIAM_BASE_URL);
    console.log("âœ… Navigated to SIAM application");
  });

  test("should have Lambda MCP transcription API endpoint", async ({ request }) => {
    console.log("ðŸ§ª Testing Lambda MCP API endpoint health...");

    const response = await request.get(`${SIAM_BASE_URL}/api/lambda-mcp/transcribe`);

    expect(response.ok()).toBeTruthy();

    const data = await response.json();
    console.log("ðŸ“Š Health check response:", data);

    expect(data.status).toBe("healthy");
    expect(data.lambdaMcp).toBeDefined();
    expect(data.statistics).toBeDefined();
  });

  test("should process audio through Lambda MCP pipeline", async ({ request }) => {
    console.log("ðŸ§ª Testing audio processing through Lambda MCP...");

    // Create a test audio file (mock audio blob)
    const audioData = createTestAudioBlob();

    const formData = new FormData();
    formData.append("audio", audioData, "test-audio.webm");
    formData.append(
      "options",
      JSON.stringify({
        enableVoiceIsolation: true,
        transcriptionModel: "gpt-4o-transcribe",
        language: "en",
      })
    );

    console.log("ðŸ“¤ Sending audio to Lambda MCP API...");

    const response = await request.post(`${SIAM_BASE_URL}/api/lambda-mcp/transcribe`, {
      multipart: {
        audio: {
          name: "test-audio.webm",
          mimeType: "audio/webm",
          buffer: Buffer.from(await audioData.arrayBuffer()),
        },
        options: JSON.stringify({
          enableVoiceIsolation: true,
          transcriptionModel: "gpt-4o-transcribe",
          language: "en",
        }),
      },
      timeout: 35000, // Account for Lambda timeout
    });

    console.log(`ðŸ“¥ Response status: ${response.status()}`);

    if (response.ok()) {
      const result = await response.json();

      console.log("âœ… Lambda MCP processing successful");
      console.log(`   Processing mode: ${result.metadata?.processingMode}`);
      console.log(`   Lambda attempted: ${result.metadata?.lambdaAttempted}`);
      console.log(`   Lambda success: ${result.metadata?.lambdaSuccess}`);
      console.log(`   Fallback used: ${result.metadata?.fallbackUsed}`);
      console.log(`   Processing time: ${result.metadata?.processingTime}ms`);

      expect(result.success).toBeTruthy();
      expect(result.transcription).toBeDefined();
      expect(result.metadata).toBeDefined();

      // Verify processing mode is lambda-mcp or hybrid (with fallback)
      expect(["lambda-mcp", "hybrid", "local"]).toContain(result.metadata.processingMode);
    } else {
      console.log("âš ï¸ Lambda MCP request failed, checking if fallback was used...");
      const errorData = await response.json();
      console.log("Error details:", errorData);

      // Fallback to local is acceptable
      if (errorData.fallbackUsed) {
        console.log("âœ… Fallback to local processing worked correctly");
        expect(errorData.fallbackUsed).toBeTruthy();
      } else {
        throw new Error(`Lambda MCP processing failed: ${errorData.error}`);
      }
    }
  });

  test("should handle Lambda timeout with fallback", async ({ request }) => {
    console.log("ðŸ§ª Testing Lambda timeout handling and fallback...");

    // Create a large audio file to potentially trigger timeout
    const largeAudioData = createLargeTestAudioBlob();

    console.log(`ðŸ“¦ Created test audio: ${(largeAudioData.size / 1024).toFixed(1)}KB`);

    const response = await request.post(`${SIAM_BASE_URL}/api/lambda-mcp/transcribe`, {
      multipart: {
        audio: {
          name: "large-test-audio.webm",
          mimeType: "audio/webm",
          buffer: Buffer.from(await largeAudioData.arrayBuffer()),
        },
        options: JSON.stringify({
          enableVoiceIsolation: true,
        }),
      },
      timeout: 60000, // Allow time for fallback processing
    });

    const result = await response.json();

    console.log(`ðŸ“Š Result metadata:`, result.metadata);

    // Should either succeed with Lambda or fall back to local
    if (result.success) {
      console.log(`âœ… Processing completed via ${result.metadata.processingMode}`);

      if (result.metadata.fallbackUsed) {
        console.log("âœ… Fallback mechanism worked correctly");
        expect(result.metadata.fallbackUsed).toBeTruthy();
        expect(result.metadata.processingMode).toBe("hybrid");
      }
    } else {
      console.log("âš ï¸ Processing failed:", result.error);
    }

    // At minimum, expect a response
    expect(result).toBeDefined();
  });

  test("should track processing metrics correctly", async ({ request }) => {
    console.log("ðŸ§ª Testing processing metrics tracking...");

    const audioData = createTestAudioBlob();

    const response = await request.post(`${SIAM_BASE_URL}/api/lambda-mcp/transcribe`, {
      multipart: {
        audio: {
          name: "test-audio.webm",
          mimeType: "audio/webm",
          buffer: Buffer.from(await audioData.arrayBuffer()),
        },
      },
      timeout: 35000,
    });

    if (response.ok()) {
      const result = await response.json();

      console.log("ðŸ“Š Processing metrics:", result.metadata?.metrics);

      expect(result.metadata).toBeDefined();
      expect(result.metadata.processingTime).toBeGreaterThan(0);

      if (result.metadata.metrics) {
        expect(result.metadata.metrics.totalLatency).toBeGreaterThan(0);

        if (result.metadata.lambdaAttempted) {
          console.log(`   Lambda latency: ${result.metadata.metrics.lambdaLatency}ms`);
          expect(result.metadata.metrics.lambdaLatency).toBeDefined();
        }

        if (result.metadata.fallbackUsed) {
          console.log(`   Local latency: ${result.metadata.metrics.localLatency}ms`);
          expect(result.metadata.metrics.localLatency).toBeDefined();
        }
      }
    } else {
      console.log("âš ï¸ Skipping metrics test - request failed");
    }
  });

  test("should perform content analysis on transcription", async ({ request }) => {
    console.log("ðŸ§ª Testing content analysis...");

    const audioData = createTestAudioBlob();

    const response = await request.post(`${SIAM_BASE_URL}/api/lambda-mcp/transcribe`, {
      multipart: {
        audio: {
          name: "test-audio.webm",
          mimeType: "audio/webm",
          buffer: Buffer.from(await audioData.arrayBuffer()),
        },
      },
      timeout: 35000,
    });

    if (response.ok()) {
      const result = await response.json();

      console.log("ðŸ” Content analysis:", result.contentAnalysis);

      expect(result.contentAnalysis).toBeDefined();
      expect(result.contentAnalysis.isExplicit).toBeDefined();
      expect(result.contentAnalysis.contentType).toBeDefined();
      expect(result.contentAnalysis.sentiment).toBeDefined();
      expect(Array.isArray(result.contentAnalysis.keywords)).toBeTruthy();

      console.log(`   Explicit: ${result.contentAnalysis.isExplicit}`);
      console.log(`   Content type: ${result.contentAnalysis.contentType}`);
      console.log(`   Sentiment: ${result.contentAnalysis.sentiment}`);
      console.log(`   Keywords: ${result.contentAnalysis.keywords.join(", ")}`);
    } else {
      console.log("âš ï¸ Skipping content analysis test - request failed");
    }
  });

  test("should get health statistics", async ({ request }) => {
    console.log("ðŸ§ª Testing health statistics endpoint...");

    const response = await request.get(`${SIAM_BASE_URL}/api/lambda-mcp/transcribe`);

    expect(response.ok()).toBeTruthy();

    const data = await response.json();

    console.log("ðŸ“Š Statistics:", data.statistics);

    expect(data.statistics).toBeDefined();
    expect(data.statistics.totalProcessed).toBeDefined();
    expect(data.statistics.lambdaSuccessCount).toBeDefined();
    expect(data.statistics.lambdaFailureCount).toBeDefined();
    expect(data.statistics.fallbackCount).toBeDefined();

    console.log(`   Total processed: ${data.statistics.totalProcessed}`);
    console.log(`   Lambda success: ${data.statistics.lambdaSuccessCount}`);
    console.log(`   Lambda failures: ${data.statistics.lambdaFailureCount}`);
    console.log(`   Fallbacks: ${data.statistics.fallbackCount}`);
    console.log(`   Success rate: ${(data.statistics.lambdaSuccessRate * 100).toFixed(1)}%`);
  });

  test("should handle cancellation gracefully", async ({ page }) => {
    console.log("ðŸ§ª Testing transcription cancellation...");

    // Navigate to chat interface
    await page.goto(`${SIAM_BASE_URL}/chat`);

    // Wait for page to load
    await page.waitForLoadState("domcontentloaded");

    console.log("âœ… Cancellation test complete (manual testing recommended for full validation)");
  });

  test("should handle large audio files with chunking", async ({ request }) => {
    console.log("ðŸ§ª Testing large audio file chunking...");

    // Create a large audio file (>5MB to trigger chunking)
    const largeAudioData = createVeryLargeTestAudioBlob();

    console.log(`ðŸ“¦ Created large audio: ${(largeAudioData.size / 1024 / 1024).toFixed(2)}MB`);

    const response = await request.post(`${SIAM_BASE_URL}/api/lambda-mcp/transcribe`, {
      multipart: {
        audio: {
          name: "very-large-test-audio.webm",
          mimeType: "audio/webm",
          buffer: Buffer.from(await largeAudioData.arrayBuffer()),
        },
        options: JSON.stringify({
          enableChunking: true,
        }),
      },
      timeout: 120000, // 2 minutes for chunked processing
    });

    const result = await response.json();

    console.log("ðŸ“Š Chunking result:", {
      success: result.success,
      processingMode: result.metadata?.processingMode,
      chunkCount: result.metadata?.chunkCount,
    });

    // Should handle large files
    expect(result).toBeDefined();

    if (result.metadata?.chunkCount) {
      console.log(`âœ… Processed ${result.metadata.chunkCount} chunks`);
      expect(result.metadata.chunkCount).toBeGreaterThan(1);
    }
  });
});

// Helper functions

function createTestAudioBlob(): Blob {
  // Create a minimal valid WebM audio blob for testing
  // This is a simplified mock - in real tests, use actual audio samples
  const audioData = new Uint8Array(1024).fill(0);
  return new Blob([audioData], { type: "audio/webm" });
}

function createLargeTestAudioBlob(): Blob {
  // Create a 2MB audio blob
  const audioData = new Uint8Array(2 * 1024 * 1024).fill(0);
  return new Blob([audioData], { type: "audio/webm" });
}

function createVeryLargeTestAudioBlob(): Blob {
  // Create a 6MB audio blob to trigger chunking
  const audioData = new Uint8Array(6 * 1024 * 1024).fill(0);
  return new Blob([audioData], { type: "audio/webm" });
}
