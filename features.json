{
  "_meta": {
    "description": "SIAM Feature Specifications - Long-Running Agent Task Tracker",
    "created": "2025-12-04",
    "version": "1.1.0",
    "rules": [
      "DO NOT remove or edit existing features without explicit user approval",
      "DO NOT mark a feature as passes:true without running verification tests",
      "Each feature must be completed one at a time",
      "Update claude-progress.txt after completing each feature",
      "Commit changes to git after each feature completion",
      "NEW: Link features to requirements via requirementRefs array",
      "NEW: Include byteroverTags for knowledge retrieval"
    ],
    "specDriven": {
      "requirementsFile": "specs/SIAM/requirements.md",
      "constitutionFile": "specs/SIAM/constitution.md",
      "featureTemplate": {
        "id": "F0XX",
        "category": "chat|curate|testing|infrastructure|documentation|data-extraction",
        "priority": 1,
        "name": "Feature Name",
        "description": "What this feature does",
        "requirementRefs": ["REQ-001", "REQ-002"],
        "byteroverTags": "#category #feature",
        "passes": false,
        "verification": ["Acceptance criteria from requirements.md"],
        "completedAt": null,
        "notes": "Additional context"
      }
    }
  },
  "features": [
    {
      "id": "F001",
      "category": "infrastructure",
      "priority": 1,
      "name": "Long-Running Agent Harness",
      "description": "Set up the Initializer/Coding Agent workflow infrastructure",
      "passes": true,
      "verification": [
        "init.sh exists and is executable",
        "claude-progress.txt exists with proper format",
        "features.json exists with proper structure"
      ],
      "completedAt": "2025-12-04T19:50:00Z",
      "notes": "Initial setup complete"
    },
    {
      "id": "F002",
      "category": "documentation",
      "priority": 2,
      "name": "Agent Workflow Documentation",
      "description": "Document the Initializer and Coding Agent workflow in CLAUDE.md or docs/",
      "passes": true,
      "verification": [
        "Documentation exists explaining the two-agent workflow",
        "Examples provided for both agent types",
        "Integration with existing SIAM workflows documented"
      ],
      "completedAt": "2025-12-04T19:55:00Z",
      "notes": "Created docs/agents/LONG-RUNNING-AGENTS.md with full workflow documentation"
    },
    {
      "id": "F003",
      "category": "data-extraction",
      "priority": 1,
      "name": "Jira/Atlassian Bulk Data Extraction",
      "description": "Extract all Jira tickets, Alexandria docs, and Atlassian data via VPN + Playwright scraping. Scheduled for 2025-12-05 15:30 Malta/Rome time.",
      "passes": false,
      "verification": [
        "VPN connection established (GlobalConnect)",
        "Playwright script navigates to Jira successfully",
        "Jira tickets extracted and saved to local JSON/CSV",
        "Alexandria documents extracted",
        "Data validated and imported into SIAM/Supabase"
      ],
      "completedAt": null,
      "notes": "Matt's machine broke - no GlobalConnect for 2 weeks. Tomorrow is the big extraction day.",
      "dependencies": ["VPN access", "Jira credentials"],
      "scheduledFor": "2025-12-05T15:30:00+01:00"
    },
    {
      "id": "F004",
      "category": "data-extraction",
      "priority": 2,
      "name": "Playwright Jira Scraper Script",
      "description": "Create Playwright script to authenticate and scrape Jira boards, tickets, comments, attachments",
      "passes": false,
      "verification": [
        "Script exists at scripts/jira-extractor.ts or similar",
        "Handles SSO/Okta authentication flow",
        "Extracts: ticket ID, title, description, status, assignee, comments, attachments",
        "Outputs structured JSON",
        "Handles pagination for large result sets"
      ],
      "completedAt": null,
      "notes": "Build before tomorrow's VPN session so we're ready to go"
    },
    {
      "id": "F005",
      "category": "data-extraction",
      "priority": 3,
      "name": "Alexandria Document Extraction",
      "description": "Extract documentation from Alexandria/Confluence via Playwright",
      "passes": false,
      "verification": [
        "Script navigates Alexandria spaces",
        "Extracts page content, hierarchy, metadata",
        "Handles embedded images and attachments",
        "Outputs markdown or structured format"
      ],
      "completedAt": null,
      "notes": "Alexandria is the Sony docs system - may be Confluence-based"
    },
    {
      "id": "F006",
      "category": "chat",
      "priority": 1,
      "name": "Query Latency Optimization",
      "description": "Reduce chat response time from ~1.8s to <1s for 95% of queries",
      "requirementRefs": ["REQ-001"],
      "byteroverTags": "#latency #chat #performance #rag",
      "passes": false,
      "verification": [
        "95th percentile response time < 1000ms",
        "Measured from chat submission to first streaming token",
        "Must touch at least 2 sources (Jira + KB or Git + KB)",
        "Playwright timing assertions pass"
      ],
      "completedAt": null,
      "notes": "Currently at 1797ms. Needs caching/optimization. See REQ-001 for full acceptance criteria."
    }
  ],
  "backlog": [
    {
      "id": "B001",
      "category": "enhancement",
      "name": "Data Import Pipeline to Supabase",
      "description": "After extraction, import Jira/Alexandria data into SIAM vector store",
      "estimatedComplexity": "medium",
      "addedAt": "2025-12-04"
    },
    {
      "id": "B002",
      "category": "enhancement",
      "name": "Incremental Sync for Jira",
      "description": "After initial bulk import, set up incremental sync to keep data fresh",
      "estimatedComplexity": "high",
      "addedAt": "2025-12-04"
    },
    {
      "id": "B003",
      "category": "data-extraction",
      "name": "Re-crawl AOMA UI with Proper Firecrawl Config",
      "description": "Requires VPN. Use Firecrawl with formats:['markdown'], onlyMainContent:true OR Playwright accessibility snapshots. Goal: capture menu labels, button names, form fields for UI-aware AI guidance. Previous crawl captured raw CSS garbage and was deleted.",
      "estimatedComplexity": "medium",
      "addedAt": "2025-12-17",
      "dependencies": ["VPN access to aoma-stage.smcdp-de.net"],
      "byteroverTags": "#firecrawl #aoma #ui-crawl #accessibility"
    },
    {
      "id": "B004",
      "category": "data-extraction",
      "name": "Re-ingest Jira with Full Ticket Content",
      "description": "Current 15,085 Jira tickets only contain titles (~78 chars avg). Need to re-extract with full descriptions, comments, and attachments for better RAG retrieval. Also re-embed with Gemini (currently 'unknown' embeddings).",
      "estimatedComplexity": "high",
      "addedAt": "2025-12-17",
      "dependencies": ["VPN access", "Jira API or Playwright extraction"],
      "byteroverTags": "#jira #embeddings #rag #data-quality"
    }
  ]
}
