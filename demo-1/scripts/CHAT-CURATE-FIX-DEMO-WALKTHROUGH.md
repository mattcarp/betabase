# ğŸ¯ SIAM Demo: Chat â†’ Curate â†’ Fix Walkthrough
**Focus:** Technical demo for knowledgeable colleagues  
**Duration:** ~10 minutes  
**Features:** Chat UI, RLHF Curation, HITL with LangGraph 1.0, Multi-Tenant AOMA Knowledge

---

## ğŸ¬ Demo Story Arc

**The Scenario:**  
A Sony Music engineer needs to understand how to create a new offering in AOMA. Instead of hunting through Confluence, JIRA, and asking colleagues, they use SIAM's AI-powered knowledge retrieval.

**What We Show:**
1. **Chat Tab** - Natural language Q&A with vector search
2. **Curate Tab** - RLHF feedback to improve responses
3. **Fix Tab** - HITL workflow when AI needs help (LangGraph 1.0)

---

## ğŸ“‹ Pre-Demo Setup Checklist

### Database State
- [ ] Supabase has AOMA knowledge vectors loaded
- [ ] Multi-tenant isolation verified (sony-music â†’ pde â†’ aoma)
- [ ] At least 50+ AOMA docs embedded (JIRA + Confluence)

### Application State
- [ ] Clean browser session (no cached auth)
- [ ] All three tabs working (Chat, Curate, Fix)
- [ ] LangGraph breakpoints configured
- [ ] Sample "fix" task in pending state

### Demo Data
- [ ] Pre-tested query: "How do I create a new offering in AOMA?"
- [ ] Pre-tested query: "What's the difference between assets and offerings?"
- [ ] Known good response sources ready
- [ ] Sample feedback already in curate queue

---

## ğŸ¤ Demo Script

### **Part 1: The Chat Tab (3 minutes)**

#### Opening (30 seconds)
> "Let me show you SIAM in action. This is our multi-tenant AI-powered knowledge base. Right now, I'm querying AOMA documentation, but the same system works for USM, DAM, or any other Sony Music appâ€”completely isolated, no cross-contamination."

#### Live Query (1 minute)
**Type into chat:**
```
How do I create a new offering in AOMA?
```

**While it's thinking, narrate:**
> "Behind the scenes, this query is:
> 1. Converted to a vector embedding (OpenAI or Gemini)
> 2. Searching our postgres vector store with HNSW indexing
> 3. Filtering by tenant: organization='sony-music', division='pde', app='aoma'
> 4. Returning top-10 most semantically relevant docs
> 5. LLM synthesizes the answer with citations"

**When response appears:**
> "Notice the response includes:
> - Direct answer to the question
> - Source citations (Confluence page IDs, JIRA tickets)
> - Confidence indicators
> - Related follow-up questions"

**Click on a source citation:**
> "These sources are realâ€”pulled from our AOMA knowledge base. Click through and you'd see the actual Confluence page or JIRA ticket."

#### Show Multi-Tenant Isolation (1 minute)
**Type second query:**
```
What's the USM session timeout default?
```

**When it returns "no results" or "not found":**
> "See that? Even though we have USM documentation in the system, it doesn't leak into AOMA queries. Complete tenant isolation. This is the multi-tenant architecture we saw in the ERDâ€”working in production."

**Switch tenant dropdown to "usm" and retry:**
> "Now if I switch to USM context and ask the same question..."

**Show it returns USM-specific results:**
> "There we goâ€”now we get USM results. Same database, same code, but logically isolated."

---

### **Part 2: The Curate Tab (3 minutes)**

#### Transition (20 seconds)
> "Great answers are one thing, but how do we make them *better*? That's where RLHF comes inâ€”Reinforcement Learning from Human Feedback. Let's look at the Curate tab."

**Click Curate tab**

#### Show Conversation Queue (1 minute)
**Point to list of conversations:**
> "This is our feedback queue. Every conversation gets logged here. Engineers can:
> - Thumbs up/down responses
> - Mark sources as helpful or not helpful
> - Flag hallucinations
> - Suggest better answers
> 
> This feedback trains the system to get better over time."

**Click on a conversation:**
> "Let me open one. Here's a conversation from yesterday where someone asked about AOMA asset metadata fields."

**Show the feedback UI:**
- Conversation history
- Response quality rating (1-5 stars)
- Source relevance checkboxes
- Free-form feedback text
- "Was this helpful?" toggle

#### Live Feedback (1 minute)
**Provide feedback on the conversation:**
> "Let's say this response was mostly good, but missed a key detail about required fields."

**Actions:**
- Rate 4/5 stars
- Check relevant sources
- Add comment: "Should mention that 'title' and 'asset_type' are required fields"
- Submit feedback

**After submission:**
> "This feedback goes into our RLHF training pipeline. The next time someone asks about AOMA assets, the system will know to emphasize required fields."

#### Show Analytics (30 seconds)
**Scroll to curate analytics section:**
> "We track:
> - Response accuracy over time
> - Most frequently corrected topics
> - Source quality by type (Confluence vs JIRA)
> - User satisfaction scores
> 
> This data drives our improvement roadmap."

---

### **Part 3: The Fix Tab (4 minutes)**

#### Transition (20 seconds)
> "Sometimes the AI gets stuck. Maybe it needs clarification, maybe it found conflicting sources, maybe the question is ambiguous. That's where Human-in-the-Loop comes inâ€”powered by LangGraph 1.0."

**Click Fix tab**

#### Show Pending Tasks (1 minute)
**Point to task queue:**
> "This is our HITL queue. These are conversations where the AI hit a breakpoint and needs human guidance."

**Show task list UI:**
- Task ID
- Original query
- Reason for HITL (ambiguous, conflicting sources, low confidence)
- Status (pending, in-progress, resolved)
- Assigned to (optional)

**Click on a task:**
> "Let me open one. This user asked: 'How do I update an offering status?'"

#### Show LangGraph State (1.5 minutes)
**Open the task detail view:**
> "Here's where LangGraph shines. We can see the entire agent state:"

**Point to state visualization:**
```json
{
  "query": "How do I update an offering status?",
  "step": "retrieval_complete",
  "retrieved_docs": [
    { "source": "AOMA-1234", "content": "...", "score": 0.82 },
    { "source": "CONF-456", "content": "...", "score": 0.79 }
  ],
  "issue": "conflicting_information",
  "details": "JIRA says use API endpoint, Confluence says use UI workflow",
  "awaiting": "human_decision"
}
```

**Narrate:**
> "The agent found two sources with conflicting info:
> - JIRA ticket says use the API endpoint
> - Confluence page describes the UI workflow
> 
> LangGraph paused at a breakpoint and asked: 'Which one should I prioritize?'"

#### Resolve the Task (1 minute)
**Show resolution UI:**
- Option 1: Pick one source (with reason)
- Option 2: Merge both (explain when to use each)
- Option 3: Escalate (mark as needs clarification)
- Option 4: Create new documentation task

**Make a decision:**
> "In this case, both are validâ€”API for automation, UI for manual updates. I'll choose 'Merge both'."

**Type resolution:**
```
Both methods are valid:
- Use API endpoint (/api/offerings/{id}/status) for automated workflows
- Use UI (Offerings â†’ Edit â†’ Status dropdown) for manual updates
Recommend showing both in the response.
```

**Click "Resolve & Continue"**

#### Show Agent Continues (30 seconds)
**Watch LangGraph resume:**
> "Now watchâ€”LangGraph takes our feedback and continues from the breakpoint."

**Show updated state:**
```json
{
  "step": "synthesis",
  "decision": "merge_sources",
  "human_feedback": "Both methods valid, show both",
  "generating_response": true
}
```

**Final response appears:**
> "And here's the updated responseâ€”now it includes both methods with clear use cases. This response will be saved and used for future similar queries."

#### Show HITL Analytics (30 seconds)
**Scroll to Fix analytics:**
> "We track:
> - Average time to resolution
> - Types of conflicts (most common)
> - Breakpoint trigger reasons
> - Response improvement after HITL
> 
> This helps us tune when to interrupt vs. let the agent decide."

---

## ğŸ¯ Closing (1 minute)

### Key Takeaways
> "So that's SIAM in action:
> 
> **Chat:** Natural language Q&A with multi-tenant vector search. Sub-200ms responses with real citations.
> 
> **Curate:** RLHF feedback loop. Every conversation makes the system smarter.
> 
> **Fix:** Human-in-the-Loop with LangGraph 1.0. AI knows when to ask for help, humans guide it, system learns.
> 
> **Multi-Tenant:** AOMA knowledge stays in AOMA. USM knowledge stays in USM. Complete isolation, same codebase."

### The Architecture Behind It
**Show ERD briefly:**
> "This all runs on the architecture we discussedâ€”3-tier multi-tenant isolation, dual embeddings, HNSW vector indexing. It's fast, scalable, and production-ready."

### Questions?
> "Questions? Want to try it yourself? Want to see the code?"

---

## ğŸ› ï¸ Technical Deep Dive (If Asked)

### Multi-Tenant Query
```typescript
// Example: How we enforce tenant isolation
const results = await supabase
  .rpc('match_siam_vectors', {
    p_organization: 'sony-music',
    p_division: 'pde',
    p_app_under_test: 'aoma',
    query_embedding: embedding,
    match_threshold: 0.78,
    match_count: 10
  });
```

### LangGraph Breakpoint
```typescript
// Example: How we define HITL breakpoints
const agent = new StateGraph({
  // ... state definition
})
  .addNode('retrieve', retrieveNode)
  .addNode('evaluate', evaluateNode)
  .addNode('hitl', humanInTheLoopNode)  // <-- breakpoint here
  .addNode('synthesize', synthesizeNode)
  .addConditionalEdges('evaluate', (state) => {
    if (state.conflictDetected) return 'hitl';  // Pause for human
    return 'synthesize';  // Continue automatically
  });
```

### RLHF Feedback Storage
```typescript
// Example: Storing feedback for training
await supabase
  .from('rlhf_feedback')
  .insert({
    conversation_id: conv.id,
    rating: 4,
    helpful_sources: ['AOMA-1234', 'CONF-456'],
    feedback_text: 'Should mention required fields',
    organization: 'sony-music',
    division: 'pde',
    app_under_test: 'aoma'
  });
```

---

## ğŸ“Š Demo Success Checklist

After the demo, you should have shown:
- âœ… Natural language query returning relevant AOMA results
- âœ… Multi-tenant isolation (AOMA â‰  USM)
- âœ… Source citations working
- âœ… RLHF feedback workflow
- âœ… LangGraph HITL breakpoint
- âœ… Human resolution of ambiguous query
- âœ… Agent resuming after human input
- âœ… Architecture overview (ERD)

---

## ğŸš¨ Troubleshooting

### Chat Returns No Results
- Check tenant context (dropdown)
- Verify database has AOMA vectors
- Check embedding service is running
- Try simpler query first

### Curate Tab Empty
- Generate some conversations first
- Check supabase connection
- Verify auth permissions

### Fix Tab Has No Tasks
- This is normal if everything's working well!
- You can manually create a test task
- Or trigger a low-confidence query

### LangGraph Won't Resume
- Check agent state in debugger
- Verify breakpoint conditions
- Look for errors in console
- Restart agent if needed

---

## ğŸ¥ Video B-Roll Ideas

- **Typing the query** in slow motion
- **Vector search visualization** (animated dots connecting)
- **Feedback stars** being clicked
- **LangGraph state** transitioning
- **ERD pulsating** connection lines
- **Code snippets** with syntax highlighting
- **Terminal output** showing vector scores

---

## ğŸ“ Follow-Up Materials

Share with attendees after:
- Link to GitHub repo (if open source)
- ERD diagram (static + animated)
- Sample queries they can try
- Architecture documentation
- LangGraph breakpoint guide
- Multi-tenant setup instructions

---

**This is a technical story, not marketing. We're showing real engineers how real tech works. No fluff. Just facts.** ğŸ’ª

Ready to practice this walkthrough? ğŸš€

